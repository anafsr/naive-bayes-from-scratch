{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes implmentation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "#we will use sklearn and scipy to track accuracy of custom calculations\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes class implementation\n",
    "\n",
    "First we will implement the custom Bayes class that is initialized with the training sets of features and outcomes.\n",
    "<br>\n",
    "\n",
    "The following methods are later embedded in the class:\n",
    "- <b>mean_var_calculate</b> - method to calclulate mean and variance of a given column vector. Returns mean and var\n",
    "- <b>norm_pdf_multivariate</b> - method to calculate value of normal multivariable pdf of a random vector $\\vec{x}$ with mean $\\mu$ and standard deviation $\\sigma^2$\n",
    "- <b>calculate_likelihood</b> - method to calculate the likelihood of a certain class given a feature vector. Calls methods <b>mean_var_calculate</b> to calculate $\\mu$ and $\\sigma^2$ for an appropriate class and <b>norm_pdf_multivariate</b> to calculate likelihood of $\\vec{x}$ distributed normally with $\\sim \\mathcal{N}(\\mu,\\,\\sigma^{2})\\$\n",
    "- <b>get_priors</b> - calculates prior class probabilities and return a dictionary of class labels as a key and corresponding prior probability as a value\n",
    "- <b>naive_bayes_classifier</b> - method to calculate predictions based on naive Bayes method. The method updates two class attributes - <b><i>probability</i></b> and <b><i>prediction</i></b>. We will update and store probability vector to use it later in probability distribution histogram. We also calculate the evidence and use complete Bayes formula normalized by the evidence as a discriminator function to obtain array of actual probabilities\n",
    "- <b>plot_prob_hist</b> - plots probability histogram. Since the threshold for classification is set to 0.5, the minimum value in range is 0.5\n",
    "- <b>calc_confusion_matrix</b> - calculates confusion matrix\n",
    "- <b>calc_accuracy</b> - calculates accuracy\n",
    "- <b>calc_error</b> - calculates error\n",
    "- <b>calc_sens</b> - calculates sensitivity\n",
    "- <b>calc_spec</b> - calculates specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Bayes class\n",
    "Class is initialized with the training set of features and outcomes\n",
    "For each outcome cathegory class implements method to calculate prior probabilities from from the Train Set\n",
    "For each outcome cathegory class implements method to calculate likelihood using norm_pdf_multivariate method\n",
    "Class implements method to calculate naive Bayess predictions for the test data\n",
    "Class implements method to calculate confusion matrix and metods to calculate classifier performance\n",
    "\"\"\"\n",
    "\n",
    "class Bayes:\n",
    "    def __init__(self, X_train, Y_train):\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.probability = []\n",
    "        self.prediction = []\n",
    "        \n",
    "    def mean_var_calculate(self, data):\n",
    "        \n",
    "        n_points = data.shape[0]\n",
    "        n_features = self.X_train.shape[1]\n",
    "        \n",
    "        mu = data.sum(axis=0)/n_points\n",
    "        sigma = np.zeros((n_features, n_features))\n",
    "        \n",
    "        x_mu = data - mu.T\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            sigma[i, i] = np.dot(x_mu[:, i].T, x_mu[:, i])\n",
    "            sigma[i, i] /= n_points\n",
    "            \n",
    "        return mu, sigma\n",
    "    \n",
    "    \"\"\"\n",
    "    Multivariate normal pdf calculation\n",
    "    Input: x = feature vector\n",
    "           mu = mean of the data\n",
    "           sigma = std of the data\n",
    "    \"\"\"\n",
    "    \n",
    "    def norm_pdf_multivariate(self, x, mu, sigma):\n",
    "    \n",
    "        dim = len(x)\n",
    "        \n",
    "        det = np.linalg.det(sigma)\n",
    "        inv = np.linalg.inv(sigma)\n",
    "        \n",
    "        norm_const = 1.0/( math.pow((2*math.pi),float(dim)/2) * math.pow(det,1.0/2) )\n",
    "        x_mu = x - mu\n",
    "        result = math.pow(math.e, -0.5 * ((np.dot(np.dot(x_mu, inv ), x_mu.T))))\n",
    "        \n",
    "        return norm_const * result\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate prior probabilites\n",
    "    Output: dictionary in the format {class, propability}\n",
    "    \"\"\"\n",
    "    def get_priors(self): \n",
    "        \n",
    "        labels, counts = np.unique(self.Y_train, return_counts=True)\n",
    "        \n",
    "        output = {}\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            output[int(label)] = float(counts[i])/len(self.Y_train)\n",
    "        return output  \n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate likelihood of feature vector x, given class label\n",
    "    Input: x = feature vector\n",
    "           label = output class\n",
    "    Output: probability x conditioned on label\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_likelihood(self, x, label):\n",
    "        \n",
    "        sub_array = np.where(self.Y_train == label)\n",
    "        mu, sigma = self.mean_var_calculate(self.X_train[sub_array])\n",
    "        \n",
    "        likelihood = self.norm_pdf_multivariate(x, mu, sigma)\n",
    "        return likelihood\n",
    "    \n",
    "    \"\"\"\n",
    "    Classifier function\n",
    "    Outputs a vector of class predictions that has the same dimension \n",
    "    as a vector of class outcomes in test data\n",
    "    \"\"\"\n",
    "    \n",
    "    def naive_bayes_classifier(self, X_test):\n",
    "        \n",
    "        assert X_test.shape[1] == self.X_train.shape[1], \"Number of features doesn't match train set!\"\n",
    "        \n",
    "        prior = self.get_priors(); \n",
    "        possible_labels = list(prior.keys());\n",
    "        self.prediction = np.zeros(len(X_test))\n",
    "        self.probability = np.zeros(len(X_test))\n",
    "            \n",
    "        for i, x_test_vector in enumerate(X_test):\n",
    "#            prediction = 0\n",
    "            evidence = sum(self.calculate_likelihood(x_test_vector, label)*prior[label] \\\n",
    "                                for label in possible_labels)\n",
    "            for label in possible_labels:\n",
    "                posterior = self.calculate_likelihood(x_test_vector, label)*prior[label]   \\\n",
    "                            / evidence \n",
    "                if posterior > self.probability[i]:\n",
    "                    self.probability[i] = posterior\n",
    "                    self.prediction[i] = label\n",
    "        return self.prediction\n",
    "    \n",
    "    \"\"\"\n",
    "    Probability histogram plot\n",
    "    Plots histogram of calculated probability values\n",
    "    based on the prediction category\n",
    "    \"\"\"\n",
    "    \n",
    "    def plot_prob_hist(self):\n",
    "        \n",
    "        for label in np.unique(self.Y_train):\n",
    "            idx = np.where(self.prediction == label)\n",
    "            plt.hist(x=self.probability[idx], bins=math.floor(len(self.probability)/10), \n",
    "                     label = \"predicted \" + str(label), alpha=0.7, rwidth=0.85)        \n",
    "        \n",
    "        plt.grid(axis='y', alpha=0.75)\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.xlabel('P of prediction'); plt.ylabel('Frequency')\n",
    "        plt.title('Probability Frequency Plot')\n",
    "        \n",
    "    \n",
    "    def calc_confusion_matrix(self, Y_pred, Y_test): #confusion matrix\n",
    "       \n",
    "        assert Y_pred.shape == Y_test.shape, \"Vectors of predictions and outcomes should be the same length!\"\n",
    "        \n",
    "        conf_mtrx = np.empty([2, 2], dtype = int)\n",
    "        \n",
    "        TN = Y_pred[(Y_pred == Y_test) & (Y_test == 0)]\n",
    "        FP = Y_pred[(Y_pred !=Y_test) & (Y_test == 0)]\n",
    "        FN = Y_pred[(Y_pred != Y_test) & (Y_test == 1)]\n",
    "        TP = Y_pred[(Y_pred == Y_test) & (Y_test == 1)]\n",
    "        \n",
    "        conf_mtrx[0,0] = len(TN); conf_mtrx[0,1] = len(FP)\n",
    "        conf_mtrx[1,0] = len(FN); conf_mtrx[1,1] = len(TP)\n",
    "        \n",
    "        return conf_mtrx\n",
    "    \n",
    "    def calc_accuracy(self, Y_pred, Y_test): #Acuracy\n",
    "        \n",
    "        conf_mtrx = self.calc_confusion_matrix(Y_pred, Y_test)\n",
    "        \n",
    "        return (conf_mtrx[0,0] + conf_mtrx[1,1])/ np.sum(conf_mtrx)\n",
    "    \n",
    "    def calc_error(self, Y_pred, Y_test): #Error rate = 1-accuracy\n",
    "        \n",
    "        conf_mtrx = self.calc_confusion_matrix(Y_pred, Y_test)\n",
    "        \n",
    "        return (conf_mtrx[0,1] + conf_mtrx[1,0])/ np.sum(conf_mtrx)\n",
    "    \n",
    "    def calc_sens(self, Y_pred, Y_test): #Sensitivity\n",
    "        \n",
    "        conf_mtrx = self.calc_confusion_matrix(Y_pred, Y_test)\n",
    "\n",
    "        return conf_mtrx[1,1]/(conf_mtrx[1,0] + conf_mtrx[1,1])\n",
    "    \n",
    "    def calc_spec(self, Y_pred, Y_test): #Specificity\n",
    "        \n",
    "        conf_mtrx = self.calc_confusion_matrix(Y_pred, Y_test)\n",
    "        \n",
    "        return conf_mtrx[0,0]/(conf_mtrx[0,0] + conf_mtrx[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the classifier\n",
    "\n",
    "Let's test the custom Bayes classifier. First we will start by defining a helper function to load the data.\n",
    "<br>\n",
    "Our helper assumes that first $(n-1)$ columns contain the feature data and the last column contains the outcome.\n",
    "<br>\n",
    "It is worth mentioning that our custom Bayes classifier takes the data on the input in the format described above. The project data is already presented in a such a way. Otherwise, consider additional preprocessing of the data before passing it to the Bayes object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Define a helper function to load the data\n",
    "    def data_loader(path): \n",
    "\n",
    "        data = pd.read_csv(path, header=None, engine='python').values\n",
    " #       data = data.drop(3, axis = 1)\n",
    "        feature = data[:, :-1]\n",
    "        outcome = data[:, -1]\n",
    "        \n",
    "        return feature, outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic set statistics\n",
    "Let's load full data set first and look at some set statistics.\n",
    "We can see from the data statistics that mean values for class '1' (diabetes) are higher than mean values for class '0' (non-diabetes). However, the standard deviations for class '1' are also higher, which implies bigger spread distribution of the data and could possibly affect correct prediction of class '1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Data statistics for outcome category '1'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.865672</td>\n",
       "      <td>141.257463</td>\n",
       "      <td>70.824627</td>\n",
       "      <td>22.164179</td>\n",
       "      <td>100.335821</td>\n",
       "      <td>35.142537</td>\n",
       "      <td>0.550500</td>\n",
       "      <td>37.067164</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.741239</td>\n",
       "      <td>31.939622</td>\n",
       "      <td>21.491812</td>\n",
       "      <td>17.679711</td>\n",
       "      <td>138.689125</td>\n",
       "      <td>7.262967</td>\n",
       "      <td>0.372354</td>\n",
       "      <td>10.968254</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.750000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.800000</td>\n",
       "      <td>0.262500</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>167.250000</td>\n",
       "      <td>38.775000</td>\n",
       "      <td>0.728000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f1          f2          f3          f4          f5          f6  \\\n",
       "count  268.000000  268.000000  268.000000  268.000000  268.000000  268.000000   \n",
       "mean     4.865672  141.257463   70.824627   22.164179  100.335821   35.142537   \n",
       "std      3.741239   31.939622   21.491812   17.679711  138.689125    7.262967   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.750000  119.000000   66.000000    0.000000    0.000000   30.800000   \n",
       "50%      4.000000  140.000000   74.000000   27.000000    0.000000   34.250000   \n",
       "75%      8.000000  167.000000   82.000000   36.000000  167.250000   38.775000   \n",
       "max     17.000000  199.000000  114.000000   99.000000  846.000000   67.100000   \n",
       "\n",
       "               f7          f8     f9  \n",
       "count  268.000000  268.000000  268.0  \n",
       "mean     0.550500   37.067164    1.0  \n",
       "std      0.372354   10.968254    0.0  \n",
       "min      0.088000   21.000000    1.0  \n",
       "25%      0.262500   28.000000    1.0  \n",
       "50%      0.449000   36.000000    1.0  \n",
       "75%      0.728000   44.000000    1.0  \n",
       "max      2.420000   70.000000    1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Data statistics for outcome category '0'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.0000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.298000</td>\n",
       "      <td>109.9800</td>\n",
       "      <td>68.184000</td>\n",
       "      <td>19.664000</td>\n",
       "      <td>68.792000</td>\n",
       "      <td>30.304200</td>\n",
       "      <td>0.429734</td>\n",
       "      <td>31.190000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.017185</td>\n",
       "      <td>26.1412</td>\n",
       "      <td>18.063075</td>\n",
       "      <td>14.889947</td>\n",
       "      <td>98.865289</td>\n",
       "      <td>7.689855</td>\n",
       "      <td>0.299085</td>\n",
       "      <td>11.667655</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.0000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25.400000</td>\n",
       "      <td>0.229750</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>107.0000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>30.050000</td>\n",
       "      <td>0.336000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>125.0000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>35.300000</td>\n",
       "      <td>0.561750</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>197.0000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>744.000000</td>\n",
       "      <td>57.300000</td>\n",
       "      <td>2.329000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               f1        f2          f3          f4          f5          f6  \\\n",
       "count  500.000000  500.0000  500.000000  500.000000  500.000000  500.000000   \n",
       "mean     3.298000  109.9800   68.184000   19.664000   68.792000   30.304200   \n",
       "std      3.017185   26.1412   18.063075   14.889947   98.865289    7.689855   \n",
       "min      0.000000    0.0000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      1.000000   93.0000   62.000000    0.000000    0.000000   25.400000   \n",
       "50%      2.000000  107.0000   70.000000   21.000000   39.000000   30.050000   \n",
       "75%      5.000000  125.0000   78.000000   31.000000  105.000000   35.300000   \n",
       "max     13.000000  197.0000  122.000000   60.000000  744.000000   57.300000   \n",
       "\n",
       "               f7          f8     f9  \n",
       "count  500.000000  500.000000  500.0  \n",
       "mean     0.429734   31.190000    0.0  \n",
       "std      0.299085   11.667655    0.0  \n",
       "min      0.078000   21.000000    0.0  \n",
       "25%      0.229750   23.000000    0.0  \n",
       "50%      0.336000   27.000000    0.0  \n",
       "75%      0.561750   37.000000    0.0  \n",
       "max      2.329000   81.000000    0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "\n",
    "df = pd.read_csv(os.path.join(path, \"data/pima-indians-diabetes.data.csv\"), header=None, engine='python').values\n",
    "df = pd.DataFrame(df, index=[i for i in range(df.shape[0])], \n",
    "                  columns=['f'+str(i+1) for i in range(df.shape[1])])\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"# Data statistics for outcome category '1'\"))\n",
    "display(df.where(df[\"f9\"]==1).describe())\n",
    "display(Markdown(\"# Data statistics for outcome category '0'\"))\n",
    "display(df.where(df[\"f9\"]==0).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will load the data into <i>train</i> and <i>test</i> sets\n",
    "\n",
    "> <b>Note</b>: In order for this section of code to run correctly create /data folder in the notebook directory and save train.csv and test.csv files inside the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = data_loader(os.path.join(path, \"data/train.csv\"))\n",
    "X_test, Y_test = data_loader(os.path.join(path, \"data/test.csv\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making custom predictions\n",
    "We now will make predictions using our custom built classifier and calculate confusion matrix of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom = Bayes(X_train, Y_train) #create custom Bayes classifier\n",
    "clf_predict=custom.naive_bayes_classifier(X_test) #predict values\n",
    "conf = custom.calc_confusion_matrix(clf_predict, Y_test) #calculate confusion matix\n",
    "custom.plot_prob_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from probability histogram plot, majority of predictions are made based on high class probabilities greater than 90%. Moreover, we can see from the plot that roughly 100 predictions out of 254 are made with probability of >94%.\n",
    "\n",
    "<br>\n",
    "Now let's display the performance numbers and examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction using custom Bayes classifier:\n",
      "\n",
      "Custom confusion matrix:\n",
      " [[132  28]\n",
      " [ 36  58]]\n",
      "\n",
      "Accuracy score (ability rate of correctly classifying cases in general):  0.7480314960629921\n",
      "Error score: (failure rate of correctly classifying cases in general) 0.25196850393700787\n",
      "Sensitivity score (ability rate of correctly classifying cases with the disease):  0.6170212765957447\n",
      "Specificity score: (ability rate of correctly classifying cases without the disease) 0.825\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction using custom Bayes classifier:\\n\")\n",
    "print('Custom confusion matrix:\\n', conf)\n",
    "print('\\nAccuracy score (ability rate of correctly classifying cases in general): ', custom.calc_accuracy(clf_predict, Y_test))\n",
    "print('Error score: (failure rate of correctly classifying cases in general)', custom.calc_error(clf_predict, Y_test))\n",
    "print('Sensitivity score (ability rate of correctly classifying cases with the disease): ', custom.calc_sens(clf_predict, Y_test))\n",
    "print('Specificity score: (ability rate of correctly classifying cases without the disease)', custom.calc_spec(clf_predict, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall our classifier achieved the accuracy score of ~75%\n",
    "<br>\n",
    "However, with this type of model we were able to achieve sensitivity score of only ~62%. In a task of identifying patients with diabetes conditions, we would probably want to achieve higher sensitivity score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating results\n",
    "\n",
    "To make sure that our custom classifier works correctly we compared our classification to classification that we obtained using scikit-learn Python library. The following results exactly matched our custom classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn confusion matrix:\n",
      " [[132  28]\n",
      " [ 36  58]]\n",
      "Accuracy score scikit-learn:  0.7480314960629921\n"
     ]
    }
   ],
   "source": [
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, Y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "\n",
    "print('Scikit-learn confusion matrix:\\n', cm)\n",
    "print('Accuracy score scikit-learn: ', accuracy_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best and the worst accuracy\n",
    "Now let's create a helper function that will randomly split data into test and validation sets and run our classifier 100 times and register best, worst and average accuracy. It is worth mentioning that initially we reserved 70% of data to the training set and got ~75% average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_function(data_x, data_y, ratio = 0.70):\n",
    "    \n",
    "    assert data_x.shape[0] == data_y.shape[0]\n",
    "    \n",
    "    train_idx = random.sample(range(data_x.shape[0]), math.floor(ratio*data_x.shape[0]));\n",
    "    test_idx = np.array([i for i in range(data_x.shape[0]) if i not in train_idx]);\n",
    "    return data_x[train_idx] , data_y[train_idx], data_x[test_idx], data_y[test_idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate average classifier accuracy using 25% of the data as a training set, specifically, 192 data points. The obtained results show less than 1% average accuracy decrease going from 70% down to 25% of the training data size. Therefore, by the nature of the classifier, we can conclude that data doesn't have outliers and and well distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = data_loader(os.path.join(path, \"data/pima-indians-diabetes.data.csv\"))\n",
    "\n",
    "acc = [] #list of all accuracies\n",
    "\n",
    "for N in range (100):\n",
    "    X_train, Y_train, X_test, Y_test = split_function(data_x, data_y, ratio = 0.25)\n",
    "    custom = Bayes(X_train, Y_train) #create custom Bayes classifier\n",
    "    clf_predict=custom.naive_bayes_classifier(X_test) #predict values\n",
    "    acc.append(custom.calc_accuracy(clf_predict, Y_test)) #append to accuracy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy:  0.78125\n",
      "Worst accuracy:  0.6927083333333334\n",
      "Average accuracy:  0.7432986111111112\n"
     ]
    }
   ],
   "source": [
    "print('Best accuracy: ', np.max(acc))\n",
    "print('Worst accuracy: ', np.min(acc))\n",
    "print('Average accuracy: ', np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
